= Rate Limiting Policy
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]
:keywords: rate limiting, api gateway, gateway, policy

[%autowidth.spread,cols="a,a"]
|===
>s| Policy name | Rate limiting
>s| Summary      | Monitors access to an API by defining a maximum amount of requests processed for a period of time
>s| Category | Quality of Service
>s| First Mule version available | v4.1.0
.4+>.^s| Returned Status Codes
|400 - Quota exceeded by WSDL APIs that use SOAP v1.2. Requests are blocked until the current window completes
|429 - Quota exceeded, requests are blocked until the current window finishes
|500 - Quota exceeded by WSDL APIs that use SOAP v1.1. Requests are blocked until the current window completes.

|===

The Rate Limiting policy enables you to control the incoming traffic to an API by limiting the number of requests that the API can receive within a given period of time. After the limit is reached, the policy rejects all requests, thereby avoiding any additional load on the backend API.

When you configure the Rate Limiting policy, you can specify any number or pairs of quota (number of requests) and time window (time period). 

Additionally, you can further configure the policy to run in a Mule cluster, or add identifiers to define groups of requests. In a clusterized policy configuration, the quota is shared between all of the nodes in the cluster. For more information about how these options work, see Examples.

== How This Policy Works

The Rate Limiting policy keeps track of the number of requests made in the current window (the available quota), allowing the requests to reach the backend only if the available quota is greater than zero. 

You can configure the policy for multiple groups of requests by using `identifiers` in the policy configuration. Each group has a separate available quota for their window. 

To understand how the Rate Limiting policy works, let’s take an example of how the configuration of 3 requests every 10 seconds allows or restricts the request, based on the quota available in that window:

image::rate-limiting-generic-example.png[Rate Limiting Policy Example,70%,85%]

In the first window, because the quota is reached with the third request, all subsequent requests are rejected until the window closes. In the second window, only 2 of the 3 requests are processed. In this window, the quota remaining for that window is dropped after the window time has elapsed.

An accepted request passes through the API to the backend. A rejected request on the other hand, displays a “429 status for HTTP, ” (or either 400 or 500 if the API is WSDL) and does not reach the backend.

== Examples

Let’s look how the same configuration of 3 requests every 10 seconds works when the configuration is clusterized and identifiers are added.

=== Configuring Identifiers with Regular String

When you add identifiers to the policy configuration from the UI, you can define groups of requests. The configured limits apply independently for each group. You can also use the identifier `#[attributes.method]` for one bucket per HTTP method in your Rate Limiting policy configuration. 

//If the universe of possible requests can be  split into a finite number of groups, then each group can be assigned with a selector key. that resolves to a selector key (a String). One Rate Limit algorithm will be created per key.  

An `Identifier` is a non-obligatory parameter.  By default the Identifier is blank. Based on how you configure the Identifier, the policy performs in the following ways:

* When not configured, the Rate Limiting quota applies to every request per bucket or group.
* When configured for an obligatory HTTP header, each case-sensitive value of the header has its own quota. 
+
Quotas are created using the _lazy creation_ strategy.
* When configured  for a non-obligatory HTTP header, custom header, payload, query parameter or expression, each value has its own quota. 
+
If the Identifier is not sent in the request, the Identifier defaults to an empty value, having its own quota. This behavior allows the Rate Limiting policy to be applied to an API consumed by uncontrolled clients, and at the same time accommodates special buckets for the clients sending the Identifier (which may be internal controlled services).

The following example shows the order of events that occur over a period of time using the Identifier `#[attributes.method]` for a limit of 3 requests every 10 seconds:

image::rate-limiting-configure-identifiers-example.png[Rate Limiting Configure Identifiers Example,70%,85%]
In the example:

* Every HTTP method is allowed 3 requests every 10 seconds (in this example only GET and POST requests have been made to the API).
* The Rate Limiting policy works in a fixed window fashion (see the window size bracket).
* The window start times are independent.
* The engine uses a _lazy creation_ strategy that spools a rate limiting algorithm whenever the first request for a method is received.

=== Configuring Identifiers with DataWeave Expressions

The rate limiting engine, which is HTTP agnostic, depends solely on the resolution of the DataWeave expression. You can alter the identifier expression to cover complex rate limiting scenarios.

For example, you can configure a Rate Limiting policy with an Identifier that uses one bucket for all class A & C LAN requests, and another bucket for everything else. The following image illustrates the latter case, which corresponds to 3 requests per 10 seconds quota with the DataWeave expression #[attributes.queryParam[‘customIdentifier’]] as the policy identifier:

image::rate-limiting-configure-identifiers-dataweave.png[Rate Limiting Configure Identifiers with DataWeave]

In the example:

* All requests without the Identifier are resolved to the empty Identifier, and therefore use a single rate limiting algorithm.
* Each different Identifier uses a different bucket, with its own independent quota.

This configuration creates a false or a true bucket that corresponds to the locality of the IP that made the request. The False and True values correspond to the domain of boolean values and not HTTP. 

Nevertheless, the policy works correctly because the engine treats the resolved expression as a String. In this case, the value is automatically cast  from Boolean to String. You can explicitly define casting in DataWeave by adding “output text/plain ---“ to your script.

[NOTE]
---
The HTTP RFC header names are case insensitive. The MuleSoft HTTP Connector changes header names to lowercase characters. However, the DataWeave is key case sensitive. Therefore, when creating the Identifier expression, remember to reference headers in lowercase.
---

=== Configuring Unbound Identifier Sets

Every Identifier result has one algorithm.You must carefully create the DataWeave expression such that it does not return an unbound or a very large co-domain, as it will require hosting the same amount of algorithms in memory (at least a request for every possible identifier has to be made as algorithms are created lazily). 

For example, suppose the DataWeave expression uses the IP as the identifier in a Mule runtime engine that is public to the Internet. If every public IPv4 IP on the internet makes a request to this Mule instance, there will be 3,706,452,992 algorithms running in a single Mule instance. 

On an average of 250 bytes per algorithm, this approximately amounts to 1 terabyte in Rate Limit algorithms. Therefore, MuleSoft recommends that you use such a DataWeave expression that resolves to a finite number of identifiers, and to keep the resulting set as small as possible.  

=== Configure Rate Limiting Policy for Clusters

Let’s take the same configuration example of 3 requests and a window that starts exactly at 12:00:00, is reset every 10 seconds, and has a 2-node Mule cluster. Both nodes start and end their windows at the same time, and the cluster allows 3 requests per window in total:

image::rate-limiting-configure-clusters.png[Rate Limting Policy Configure Clusters]

As the policy is clusterized, the whole cluster accepts 3 requests. If the clusterizable policy is turned off, the Mule cluster can accept 6 requests per window, that is 3 requests per node.

Distributed counters impact performance due to the need for synchronization between the nodes. The policy uses caching mechanisms to predict the behavior and maximize throughput. However, in a worst case scenario, you can expect higher latency. Therefore, whether you use clusterizable configuration must depend solely on your use case.

[NOTE]
---
For this feature, the Mule runtime engine instance must be running as part of a Mule cluster.
---

== Configuring the Policy

When you configure your Rate Limiting policy, you must consider certain aspects of your environment to help you derive the most value from the policy, as explained in this section.

=== Cluster or Standalone

You might have decentralized processing in your environment, with the following setup:

* N servers for the same API
* Each server has its own backend
* The amount of requests that can be served is limited only by the backend
* Fastest response time

In such a scenario, you do not need to run the policy in a clustered setup. Simply set the policy limits of the policy lower than the backend capacity of the weakest of the nodes. Additionally, a load balancer might be useful in case a node goes down.	

On the other hand, if you have centralized processing in your environment, with the following setup:

* N servers for the same API
* Single backend to which all of the proxies connect
* Possible load balancer in front of the proxies

In such a scenario, you do not need a cluster. If q is the maximum capacity of the backend, then configure the policy as follows:  where  is a small number, lower than the backend capacity.

If there are no load balancers in your environment, the cluster mode is recommended over a standalone, because you cannot configure beforehand how much traffic each node will handle. The Rate Limiting policy is designed to work both on a perfectly balanced workload, or on a completely uneven one. The backend does not receive any extra requests and thus does not exceed the maximum capacity that it can handle.

=== Window Sizes in Clusters

In clusters, the nodes must share information for consistency across the cluster. The sharing process adds latency that must be taken into account when reviewing performance.

In a worst case scenario, the number of penalized requests with latency due to cluster consistency is constant and independent from the actual size of the configured quota. Consequently, the smaller the window, the greater the percentage of potentially delayed requests. 

Therefore, MuleSoft recommends that you set up only window sizes greater than one minute in Rate Limiting and Rate Limiting SLA policy configurations for the cluster scenario.

=== Persistence

You can configure Rate Limiting policy to use big windows sizes, such asdays, months, and years. For example, suppose as a client you want to allow your user ‘X’ to consume 1 million requests per year. You cannot predict whether the node will be up the entire period or will need maintenance, which may result in restarting the Mule runtime engine. 

The algorithm has been running for several months, so the client will lose critical information. Persistence solves this problem by periodically saving the current policy state. In case of a redeployment or a restart, the algorithms are recreated from the last known persisted state or started from a clean state.

Although persistence is enabled by default, you can turn it off by setting the following property to false:

`throttling.persistence_enabled`

You can also tweak the persistence frequency rate, which has a default of 10 seconds:
throttling.persistent_data_update_freq

[Note]
---
Persistence is not available on CloudHub.
---

== Configuring Policy Parameters

When you apply the Rate Limiting policy to your API from the UI, you can configure the following parameters:

[%header%autowidth.spread,cols="a,a,a"]
|===
|Parameter | Description | Example
|Identifier | Specifies a selector key using a Dataweave or regular string | `#[attributes.method]` This creates one group for each available method in HTTP, and for example, the policy will rate limit GET requests independently from POST requests.
|# of Reqs | The quota available per window | A positive number, for example: 3.
|Time Period | The amount of time for which the quota is to be applied | A positive number, for example: 10
|Time Unit | The time unit for the Time Period in milliseconds, seconds, minutes, or hours |Minutes
|Clusterizable | Shares the quota defined for a group among all of the nodes of a Mule cluster | checked/unchecked
| Expose Headers| Defines whether to expose the  <<x-ratelimit headers,response-header>> as part of the response | checked/unchecked

|===

== FAQ

*When does the window start?*
The window starts on the first request after the policy has been successfully applied.

*What type of window does the algorithm use?*
It uses a fixed window.

*What happens when the quota is exhausted?*
The algorithm is created on demand, when the first request is received. This event fixes the time window. Each request consumes quota from the current window until the time expires. 

When the quota is exhausted, the Rate Limiting policy rejects the request. When the time window closes, the quota is reset and a new window of the same fixed size starts.

*What happens if I define multiple limits?*
The policy creates one algorithm for each limit with its quota per time window configuration. Therefore, when multiple limits are configured, every algorithm must have available quota in its current window for the request to be accepted.

[[response-header]]
*What does each response header mean?*
Each response header has information about the current state of the request:

* X-Ratelimit-Remaining: The amount of available quota
* X-Ratelimit-Limit: The maximum available requests per window
* X-Ratelimit-Reset: The remaining time, in milliseconds, until a new window starts

MuleSoft advises that you expose these headers only if the API is used within the organization. In a public API scenario, this data can be used maliciously to timely burst in traffic that leaves your API without any quota left.

*Can I configure a Mule Cluster in CloudHub?*
No, the feature is available only for RTF, Hybrid and Standalone Mule setups.

*When should I use Rate Limiting instead of Rate Limiting SLA or Spike Control?*
Rate Limiting and Rate Limiting SLA policies must be used for accountability and to enforce a hard limit to a group (using the Identifier in Rate Limiting) or to a Client application (using Rate Limiting SLA). If you want to protect your backend, use the Spike Control policy instead.


== See Also

* xref:tutorial-manage-an-api.adoc[Applying a Policy and SLA Tier]
* xref:delete-sla-tier-task.adoc[Removing SLA Tiers on API Manager]
* xref:resource-level-policies-about.adoc[Reviewing Resource Level Policies]